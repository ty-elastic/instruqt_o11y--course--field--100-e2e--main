consts:
  ai_connector: Elastic-Managed-LLM
  ai_proxy: https://tbekiares-demo-aiassistantv2-1059491012611.us-central1.run.app
  ai_timeout: 10m
  es_host: https://snap2-96da6f.es.us-west2.gcp.elastic-cloud.com
  kbn_auth: ApiKey SVdFNU81b0J4Wi1oTDdBWnR5Q3I6RmhNY0pub2pwOHFwTEFfa0d4SWtldw==
  kbn_host: https://snap2-96da6f.kb.us-west2.gcp.elastic-cloud.com
name: case_process
version: '1'
description: Process o11y cases
enabled: true
tags:
- automated_observability_triage
settings:
  timeout: 20m
triggers:
- enabled: true
  type: manual
inputs:
- name: case_id
  type: string
steps:
- name: debug2
  type: console
  with:
    message: '{{ inputs | json }}'
- name: get_case
  type: kibana.request
  with:
    headers:
      Authorization: '{{ consts.kbn_auth }}'
    method: GET
    path: /api/cases/{{ inputs.case_id }}
- name: get_case_alerts
  type: kibana.request
  with:
    headers:
      Authorization: '{{ consts.kbn_auth }}'
    method: GET
    path: /api/cases/{{ inputs.case_id }}/alerts
- name: debug3
  type: console
  with:
    message: '{{ steps.get_case_alerts.output | json }}'
- name: get_case_comments
  type: kibana.request
  with:
    headers:
      Authorization: '{{ consts.kbn_auth }}'
    method: GET
    path: /api/cases/{{ inputs.case_id }}/comments/_find
- name: debug1
  type: console
  with:
    message: '{{ steps.get_case.output | json }}'

- name: foreach_custom_field
  type: foreach
  foreach: '{{ steps.get_case.output.customFields | json }}'
  steps:
    - name: is_system_field
      type: if
      condition: 'foreach.item.key : system'
      steps:
        - name: get_topology
          type: http
          with:
            body:
              size: 1
              query:
                term:
                  system:
                    value: '{{ foreach.item.value }}'
              sort:
                "@timestamp":
                  order: desc
            headers:
              Authorization: '{{ consts.kbn_auth }}'
              Content-Type: application/json
            method: POST
            url: '{{ consts.es_host }}/workflow_topology/_search'

- name: rca1
  type: http
  with:
    body:
      connectorId: '{{ consts.ai_connector }}'
      messages:
      - '@timestamp': now
        message:
          role: user
          content: |
            We will be performing a multi-step Root Cause Analysis. 
            During your analysis, use the provided incident/case, consider the alerts attached to the case and any comments in the case, and the attached topology to understand dependencies. 
            When looking for problems, always start at the service which is closest to or is a leaf node and is alerting. 
            Do not start the RCA analysis; we will be doing it together step-by-step.
            First, I will be providing you with related information you can use during this analysis. Do not take any further action until directed.

      - '@timestamp': now
        message:
          role: user
          content: |
            # Related Case
            ```
            {{ steps.get_case.output | json }}
            ```

      - '@timestamp': now
        message:
          role: user
          content: |
            # Related Alerts
            ```
            {{ steps.get_case_alerts.output | json }}
            ```

      - '@timestamp': now
        message:
          role: user
          content: |
            # Related Comments
            ```
            {{ steps.get_case_comments.output | json }}
            ```

      - '@timestamp': now
        message:
          role: user
          content: |
            # System Topologies
            ```
            {{ steps.get_topology.output.data.hits.hits[0]._source | json }}
            ```

      - '@timestamp': now
        message:
          role: user
          content: |
            Using the toplogies, can you identify the service which is both closet to being a leaf node AND has an active alert associated with it? Output a field 'service_name' with the determined service name. Output a field 'host_name' with the name of the host that the service is running on.
            Additionally, what is a good time window to search for problems before and after the associated active alert? Output a field 'start_time' with the determined start time in ISO format and a field 'end_time' with the determined end time in ISO format.
      output: false
      persist: true
    headers:
      Content-Type: application/json
      kibana-auth: '{{ consts.kbn_auth }}'
      kibana-host: '{{ consts.kbn_host }}'
    method: POST
    timeout: '{{ consts.ai_timeout }}'
    url: '{{ consts.ai_proxy }}/api/observability_ai_assistant/chat/complete'

- name: relevant_logs
  type: http
  with:
    body:
      query: |
        FROM logs-*
        | WHERE @timestamp >= "{{ steps.rca1.output.data.result.start_time}}" AND @timestamp <= "{{ steps.rca1.output.data.result.end_time}}"
        | WHERE service.name == "{{ steps.rca1.output.data.result.service_name}}"
        | WHERE TO_UPPER(log.level) IN ("WARN", "WARNING", "SEVERE", "FATAL", "ERROR") 
        | STATS pattern = COUNT() BY CATEGORIZE(message), BUCKET(@timestamp, 10m)
        | LIMIT 1000
    headers:
      Authorization: '{{ consts.kbn_auth }}'
      Content-Type: application/json
    method: POST
    url: '{{ consts.es_host }}/_query?format=txt'

- name: relevant_host_metrics
  type: http
  with:
    body:
      query: |
        FROM metrics-* 
        | WHERE @timestamp >= "{{ steps.rca1.output.data.result.start_time}}" AND @timestamp <= "{{ steps.rca1.output.data.result.end_time}}"
        | WHERE host.name == "{{ steps.rca1.output.data.result.host_name}}"
        | WHERE system.cpu.utilization IS NOT NULL OR `system.cpu.load_average.1m` IS NOT NULL OR system.memory.utilization IS NOT NULL OR system.cpu.logical.count IS NOT NULL OR system.filesystem.usage IS NOT NULL
        | STATS 
          mem_used = AVG(system.memory.utilization) WHERE state == "used", mem_buffered = AVG(system.memory.utilization) WHERE state == "buffered", mem_reclaimable = AVG(system.memory.utilization) WHERE state == "slab_reclaimable", mem_unreclaimable = AVG(system.memory.utilization) WHERE state == "slab_unreclaimable", 
          load_1m = AVG(`metrics.system.cpu.load_average.1m`), cpucount = MAX(system.cpu.logical.count), 
          cpu_idle = AVG(system.cpu.utilization) WHERE state == "idle", cpu_wait = AVG(system.cpu.utilization) WHERE state == "wait",
          free_fs = SUM(system.filesystem.usage) WHERE state == "free", total_fs = SUM(system.filesystem.usage)
          BY BUCKET(@timestamp, 1m)
        | EVAL cpu_usage = (1 - (cpu_idle + cpu_wait)) * 100
        | EVAL normalized_load = (TO_DOUBLE(load_1m) / TO_DOUBLE(cpucount)) * 100
        | EVAL mem_usage = (mem_used + mem_buffered + mem_reclaimable + mem_unreclaimable) * 100
        | EVAL disk_usage = (1 - TO_DOUBLE(free_fs) / TO_DOUBLE(total_fs)) * 100
        | KEEP cpu_usage, normalized_load, mem_usage, disk_usage, `BUCKET(@timestamp, 1m)`
        | LIMIT 1000
    headers:
      Authorization: '{{ consts.kbn_auth }}'
      Content-Type: application/json
    method: POST
    url: '{{ consts.es_host }}/_query?format=txt'

- name: relevant_trace_metrics
  type: http
  with:
    body:
      query: |
        FROM traces-*
        | WHERE @timestamp >= "{{ steps.rca1.output.data.result.start_time}}" AND @timestamp <= "{{ steps.rca1.output.data.result.end_time}}"
        | WHERE service.name == "{{ steps.rca1.output.data.result.service_name}}"
        | STATS 
          spans = COUNT(), 
          failed_spans = COUNT() WHERE event.outcome != "success",
          latency_us = AVG(span.duration.us)
          BY span.subtype, span.name, BUCKET(@timestamp, 1m)
        | EVAL failed_span_percent = (TO_DOUBLE(failed_spans) / TO_DOUBLE(spans)) * 100
        | LIMIT 1000
    headers:
      Authorization: '{{ consts.kbn_auth }}'
      Content-Type: application/json
    method: POST
    url: '{{ consts.es_host }}/_query?format=txt'

- name: rca5
  type: http
  with:
    body:
      result: False
      connectorId: '{{ consts.ai_connector }}'
      conversationId: '{{ steps.rca1.output.data.conversationId }}'
      messages:
      - '@timestamp': now
        message:
          role: user
          content: |
            I will now be providing you with related telemetry you can use when asked to perform Root Cause Analysis. Do not take any further action until directed.

      - '@timestamp': now
        message:
          role: user
          content: |
            # Relevant Logs
            ```
            {{ steps.relevant_logs.output.data }}
            ```
      - '@timestamp': now
        message:
          role: user
          content: |
            # Relevant Host Metrics
            ```
            {{ steps.relevant_host_metrics.output.data }}
            ```
      - '@timestamp': now
        message:
          role: user
          content: |
            # Relevant Trace Metrics
            ```
            {{ steps.relevant_trace_metrics.output.data }}
            ```
      - '@timestamp': now
        message:
          role: user
          content: |
            Considering the provided topology, the provided alerts, and the provided relevant telemetry, please output a detailed Root Cause Analysis.
      output: true
      persist: true
    headers:
      Content-Type: application/json
      kibana-auth: '{{ consts.kbn_auth }}'
      kibana-host: '{{ consts.kbn_host }}'
    method: POST
    timeout: '{{ consts.ai_timeout }}'
    url: '{{ consts.ai_proxy }}/api/observability_ai_assistant/chat/complete'

- name: update_ai_conversation_id
  type: kibana.request
  with:
    body:
      cases:
        - id: '{{ inputs.case_id }}'
          version: '{{ steps.get_case.output.version }}'
          customFields:
            - key: "ai_conversation_id"
              type: "text"
              value: '{{ steps.rca1.output.data.conversationId }}'
            - key: "system"
              type: "text"
              value: '{{ steps.get_topology.output.data.hits.hits[0]._source.system }}'
    headers:
      Authorization: '{{ consts.kbn_auth }}'
      Content-Type: application/json
      kbn-xsrf: true
    method: PATCH
    path: /api/cases

- name: add_comment_to_existing_case
  type: kibana.request
  with:
    body:
      comment: '{{ steps.rca5.output.data.lastMessage }}'
      owner: observability
      type: user
    headers:
      Authorization: '{{ consts.kbn_auth }}'
      Content-Type: application/json
      kbn-xsrf: true
    method: POST
    path: /api/cases/{{ inputs.case_id }}/comments

- name: add_conversation_to_existing_case
  type: kibana.request
  with:
    body:
      comment: You can continue your investigation [here](/app/observabilityAIAssistant/conversations/{{ steps.rca1.output.data.conversationId }}).
      owner: observability
      type: user
    headers:
      Authorization: '{{ consts.kbn_auth }}'
      Content-Type: application/json
      kbn-xsrf: true
    method: POST
    path: /api/cases/{{ inputs.case_id }}/comments


